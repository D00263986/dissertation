{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f897645-3f8e-471a-a58f-11b6534a3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, DenseNet121, Xception, MobileNetV2, EfficientNetB0, NASNetMobile, InceptionResNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Global variables\n",
    "max_images_per_breed = 300\n",
    "total_epochs = 10\n",
    "\n",
    "def load_image_paths_and_labels(directory, max_images_per_breed):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for breed in os.listdir(directory):\n",
    "        breed_path = os.path.join(directory, breed)\n",
    "        if os.path.isdir(breed_path):\n",
    "            count = 0\n",
    "            for filename in os.listdir(breed_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(breed_path, filename))\n",
    "                    labels.append(breed)\n",
    "                    count += 1\n",
    "                    if count >= max_images_per_breed:\n",
    "                        break\n",
    "    return image_paths, labels\n",
    "\n",
    "def train(model_type='vgg16'):\n",
    "    # Directories\n",
    "    base_dir = os.path.dirname(os.path.abspath('vgg16.ipynb'))\n",
    "    thesis_dir = os.path.abspath(os.path.join(base_dir, \"..\"))\n",
    "    models_dir = os.path.join(thesis_dir, 'models')\n",
    "    class_indices_file = os.path.join(thesis_dir, 'class_indices.json')\n",
    "\n",
    "    train_dir = os.path.join(thesis_dir, 'images', 'train')\n",
    "    val_dir = os.path.join(thesis_dir, 'images', 'val')\n",
    "\n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(train_dir):\n",
    "        raise FileNotFoundError(f\"Training directory not found: {train_dir}\")\n",
    "    if not os.path.exists(val_dir):\n",
    "        raise FileNotFoundError(f\"Validation directory not found: {val_dir}\")\n",
    "\n",
    "    # Load class indices\n",
    "    if not os.path.exists(class_indices_file):\n",
    "        raise FileNotFoundError(f\"Class indices file not found: {class_indices_file}\")\n",
    "\n",
    "    with open(class_indices_file, 'r') as f:\n",
    "        class_indices = json.load(f)\n",
    "\n",
    "    # Data Augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    # Load base model\n",
    "    base_model = None\n",
    "    if model_type == 'vgg16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'resnet50':\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'inceptionv3':\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'densenet121':\n",
    "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'xception':\n",
    "        base_model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'mobilenetv2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'efficientnetb0':\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'nasnetmobile':\n",
    "        base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    elif model_type == 'inceptionresnetv2':\n",
    "        base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    else:\n",
    "        raise ValueError(f\"Model type {model_type} is not supported\")\n",
    "\n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom layers\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(len(class_indices), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Checkpoint callback with the correct file extension\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        os.path.join(models_dir, f'{model_type}.keras'),  # Change to .keras\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=total_epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=len(val_generator),\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    # Plot training & validation accuracy and loss\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_type} Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_type} Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(models_dir, f'{model_type}_training_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    val_labels = val_generator.classes\n",
    "    val_predictions = model.predict(val_generator)\n",
    "    val_predictions = np.argmax(val_predictions, axis=1)\n",
    "    conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=list(class_indices.keys()), yticklabels=list(class_indices.keys()))\n",
    "    plt.title(f'{model_type} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(os.path.join(models_dir, f'{model_type}_confusion_matrix.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f947e76-98a0-4f0a-bbcd-2158aa977144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9025 images belonging to 10 classes.\n",
      "Found 3929 images belonging to 10 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=C:\\Users\\josej\\Thesis\\models\\vgg16.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 133\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model_type)\u001b[0m\n\u001b[0;32m    126\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    127\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m),\n\u001b[0;32m    128\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    129\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Checkpoint callback\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m    134\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    135\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    136\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    137\u001b[0m )\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    140\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    141\u001b[0m     train_generator,\n\u001b[0;32m    142\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_generator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint]\n\u001b[0;32m    147\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[1;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=C:\\Users\\josej\\Thesis\\models\\vgg16.h5"
     ]
    }
   ],
   "source": [
    "train('vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacf8bd-9eb2-4815-ab5e-da314c745e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759bc78-aa15-43c8-a916-0f58f70bfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('inceptionv3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd733be-56c5-42ed-a4bd-a824ce3f42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('xception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e8f65-d61e-42c2-b7c6-d6ac65f58b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('densenet121')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78c7cd-9a22-4978-b294-6623f4e96365",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('mobilenetv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c87680-5501-4ae9-b13e-e47f0b479c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('nasnetmobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf94a06-ee52-49f9-8cce-cad9f7946ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('efficientnetb0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46debbf8-be19-4965-a583-13b1de0df3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('inceptionresnetv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef3614-f0b1-4f85-bc4c-0970d8054ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('vgg19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae11ad6-ffdb-46bb-ab6a-462204e3bf51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec745534-988d-46b9-a6dc-bc87ef5f722d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9499c83-985a-4a8b-9041-b78850932a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554c8fd-0232-4d06-b320-6e7cda0fdd38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272d0ba-00a9-4386-b5d6-48c7e688ac47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
